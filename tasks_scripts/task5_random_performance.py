"""
Task 5: Core Prediction Logic for Scaled Evaluation

This script contains the final, robust version of the NER prediction pipeline.
While the main evaluation loop for Task 5 is in another file, this script
provides the core function, `label_text_with_llm`, which is called for each
of the 50 random files during the evaluation.

Key features of this script include:
- Handling long documents by splitting them into overlapping chunks.
- Using a refined, keyword-based mapping to improve label accuracy.
- Removing duplicate predictions that arise from the chunking process.
"""

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

def get_refined_label(model_label, entity_text):
    """
    Maps the model's raw output labels to the required assignment categories.

    This function acts as a rule-based layer on top of the model. It uses
    keyword matching to differentiate a generic 'problem' into a more specific
    'ADR', 'Disease', or 'Symptom'.

    Args:
        model_label (str): The raw label predicted by the model (e.g., 'problem').
        entity_text (str): The text of the entity being classified.

    Returns:
        str or None: The refined label ('ADR', 'Disease', 'Symptom', 'Drug') or
                     None if the label is not one of the target categories.
    """
    # Simple keyword lists for a heuristic-based classification.
    ADR_KEYWORDS = [
        'dizzy', 'nausea', 'rash', 'drowsy', 'headache', 'blurred vision', 
        'gastric', 'weird', 'agony', 'dizziness'
    ]
    DISEASE_KEYWORDS = [
        'arthritis', 'cancer', 'diabetes', 'hypertension', 'infection'
    ]

    # --- Mapping Logic ---
    if model_label == 'treatment':
        return 'Drug'
    
    if model_label == 'problem':
        lower_text = entity_text.lower()
        if any(keyword in lower_text for keyword in ADR_KEYWORDS):
            return 'ADR'
        if any(keyword in lower_text for keyword in DISEASE_KEYWORDS):
            return 'Disease'
        return 'Symptom'
        
    # Ignore other labels predicted by the model (e.g., 'person', 'pronoun').
    return None

def label_text_with_llm(text_path):
    """
    Performs end-to-end NER on a given text file, with support for long documents.

    Args:
        text_path (str): The full path to the input .txt file.

    Returns:
        list: A list of unique, refined dictionaries for each predicted entity.
    """
    try:
        # Load the model from a local path to ensure reliability and consistency.
        model_name = "/kaggle/input/bert-ner-final-model" 
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForTokenClassification.from_pretrained(model_name)
    except Exception as e:
        print(f"Error loading model or tokenizer: {e}")
        return []

    with open(text_path, 'r', encoding='utf-8') as f:
        post_text = f.read()

    # Tokenize the text using a sliding window strategy to handle long inputs.
    inputs = tokenizer(post_text, return_tensors="pt", 
                       max_length=512,                     # Max length of each chunk.
                       truncation=True,                    # Explicitly enable truncation.
                       stride=128,                         # Number of overlapping tokens between chunks.
                       return_overflowing_tokens=True)   # Return all chunks, not just the first.
    
    id_to_label = model.config.id2label
    all_raw_labels = []

    # Process each chunk generated by the tokenizer.
    for i in range(inputs.input_ids.shape[0]):
        chunk_input_ids = inputs.input_ids[i].unsqueeze(0)
        chunk_attention_mask = inputs.attention_mask[i].unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(input_ids=chunk_input_ids, attention_mask=chunk_attention_mask)

        predictions = torch.argmax(outputs.logits, dim=2)[0]
        
        # Aggregate entities from the current chunk.
        current_entity_token_ids = []
        current_entity_label = None

        for token_idx, token_id in enumerate(chunk_input_ids[0]):
            token_label_str = id_to_label[predictions[token_idx].item()]

            if token_label_str.startswith('B_'):
                if current_entity_token_ids:
                    entity_text = tokenizer.decode(current_entity_token_ids)
                    all_raw_labels.append({'label': current_entity_label, 'text': entity_text})
                current_entity_token_ids = [token_id.item()]
                current_entity_label = token_label_str[2:]
            elif token_label_str.startswith('I_') and current_entity_label == token_label_str[2:]:
                current_entity_token_ids.append(token_id.item())
            else:
                if current_entity_token_ids:
                    entity_text = tokenizer.decode(current_entity_token_ids)
                    all_raw_labels.append({'label': current_entity_label, 'text': entity_text})
                current_entity_token_ids = []
                current_entity_label = None
        
        if current_entity_token_ids:
            entity_text = tokenizer.decode(current_entity_token_ids)
            all_raw_labels.append({'label': current_entity_label, 'text': entity_text})

    # --- Final cleanup with refined mapping and deduplication ---
    cleaned_labels = []
    # Use a set to automatically handle duplicate entities that may have been
    # detected in the overlapping regions of the text chunks.
    unique_entities = set()

    for label_info in all_raw_labels:
        refined_label = get_refined_label(label_info['label'], label_info['text'])
        if refined_label:
            entity_tuple = (refined_label, label_info['text'].strip())
            if entity_tuple not in unique_entities:
                cleaned_labels.append({'label': refined_label, 'text': entity_tuple[1]})
                unique_entities.add(entity_tuple)
    
    return cleaned_labels